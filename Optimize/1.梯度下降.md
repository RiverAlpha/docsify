# 梯度下降

[视频资源](https://www.bilibili.com/video/BV18P4y1j7uH/?spm_id_from=333.337.search-card.all.click&vd_source=6821be43ebeb22bef47fed51f9cc29c2)

在线性模型中，根据相关公式，可以直接把斜率（参数）和截距一步到位算出来，但是如果数据够大牵扯到的内存需求就会非常大，所以通常不用公式一步到位。所以引入本节的梯度下降算法进行优化。

还是从拟合二维空间的点为线性模型开始，算法先预设一个斜率$k$和一个截距$b$，然后计算每个样本的预测值，然后用MSE方法算出这个模型的方差代表精准度。
$$
MSE = (y_{1实际}-y_{1预测})^2+(y_{2实际}-y_{2预测})^2+...
$$
MSE可以体现误差总体大小。

因为有无数条直线供选择，而我们选择方差最小的直线。这意味着每条直线有自己的斜率$k$和截距$b$以及对应的$MSE$.所以从数学角度出发可以表示为
$$MSE = f(k,b)$$
而这在数学中其实就是一个曲面，而曲面最低点就是MSE最小的点，此时的k,b就是最佳参数。

现在的问题是，初始化的点并不一定就是最低点，如何通过这个点到达最低点呢？

学过导数都知道，如果一条线的斜率是正数那么往前走就是走上坡路，往后走就是走下坡路，这里我们有k方向和b方向，比如你的东边是k正方向，北边是b正方向，那么如果这两个方向的导数都是负数那么最低点很有可能就在你的东北方向。所以应该往东北方向走。而且如果北边的坡更陡表示东北往北再偏一点会更快到达山谷（最低点）。所以导数的大小这时候对方向的决策就有用了。以此我们引出梯度下降公式。
$$k_{new} = k_{old} - \eta \cdot \nabla J_k$$
$$b_{new} = b_{old} - \eta \cdot \nabla J_b$$
这里的$\nabla J_k$就是k方向的导数，$\eta$表示学习率，这个值越大新的k值和老的k值就会差的越大，类比之下就是你的步子越大，那为什么是减而不是加呢，因为导数具有方向性如果是正的往前走就会往山顶走，我们目标是往山底走，所以如果导数是正数我们就往后走（减掉了），如果导数是负数那就往前走（减一个负数相当于加一个正数）

如果将k和b合成一起就变成了向量梯度了，这样表示
$$\nabla f = \begin{bmatrix} \frac{\partial f}{\partial k} \\ \frac{\partial f}{\partial b} \end{bmatrix} $$
这样就可以看成整体加减运算了。

# 随机梯度下降
@todo
# 小批量梯度下降
@todo