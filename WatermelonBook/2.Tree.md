# 树模型

有如下数据。

| ID | 年龄 ($A_1$) | 有工作 ($A_2$) | 有自己的房子 ($A_3$) | 信贷情况 ($A_4$) | 类别（是否放贷） ($Y$) |
| :---: | :---: | :---: | :---: | :---: | :---: |
| 1 | 青年 | 否 | 否 | 一般 | 否 |
| 2 | 青年 | 否 | 否 | 好 | 否 |
| 3 | 青年 | 是 | 否 | 好 | 是 |
| 4 | 青年 | 是 | 是 | 一般 | 是 |
| 5 | 青年 | 否 | 否 | 一般 | 否 |
| 6 | 中年 | 否 | 否 | 一般 | 否 |
| 7 | 中年 | 否 | 否 | 好 | 否 |
| 8 | 中年 | 是 | 是 | 好 | 是 |
| 9 | 中年 | 否 | 是 | 非常好 | 是 |
| 10 | 中年 | 否 | 是 | 非常好 | 是 |
| 11 | 老年 | 否 | 是 | 非常好 | 是 |
| 12 | 老年 | 否 | 是 | 好 | 是 |
| 13 | 老年 | 是 | 否 | 好 | 是 |
| 14 | 老年 | 是 | 否 | 非常好 | 是 |
| 15 | 老年 | 否 | 否 | 一般 | 否 |

若现在给定一个数据

| ID | 年龄 ($A_1$) | 有工作 ($A_2$) | 有自己的房子 ($A_3$) | 信贷情况 ($A_4$) | 
| :---: | :---: | :---: | :---: | :---: |
| 16 | 青年 | 是 | 否 | 一般 |

根据现有数据，你会给这个人放贷吗，如果量化下来，你给他放贷的概率是多少

## 1.前导知识
### 信息增益 (Information Gain)

尽管看似这些属性在同一水平，但是在做决定的时候还是要从某个属性开始，比如极端情况下某一个属性就直接一票否决了，而把这个属性放在最后考虑就浪费时间了，所以对属性进行排序考虑是必要的，如何用数学来量化哪个属性更重要呢？这涉及到了信息论里面的信息增益。


####  熵 (Entropy)
熵是表示随机变量不确定性的度量。设 $X$ 是一个取有限个值的离散随机变量，其概率分布为 $P(X=x_i) = p_i$，则随机变量 $X$ 的熵定义为：

$$
H(X) = -\sum_\{i=1\}^n p_i \log_2 p_i
$$

* **直观理解**：熵越大，随机变量的不确定性就越大（越混乱）。
* **规定**：若 $p_i = 0$，则 $0 \log 0 = 0$。

对于我们的数据集 $D$，假设有 $K$ 个类别 $C_k$（如“是”和“否”），样本总数为 $|D|$，第 $k$ 类样本数为 $|C_k|$，则数据集 $D$ 的**经验熵** $H(D)$ 为：

$$
H(D) = -\sum_\{k=1\}^K \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|} \tag{5.1}
$$

> **💡 例子**：在贷款数据中，总样本 $|D|=15$，“是”有 9 个，“否”有 6 个。
> $$H(D) = -(\frac{9}{15}\log_2\frac{9}{15} + \frac{6}{15}\log_2\frac{6}{15}) = 0.971$$

#### 条件熵 (Conditional Entropy)
条件熵 $H(Y|X)$ 表示在已知随机变量 $X$ 的条件下，随机变量 $Y$ 的不确定性。

对应到决策树中，假设特征 $A$ 有 $n$ 个可能的取值 $\{a_1, a_2, \dots, a_n\}$（比如“年龄”有青年、中年、老年 3 个取值）。
我们将数据集 $D$ 划分为 $n$ 个子集 $D_1, D_2, \dots, D_n$，其中 $|D_i|$ 为子集 $D_i$ 的样本个数。

则特征 $A$ 对数据集 $D$ 的**经验条件熵** $H(D|A)$ 定义为：

$$
H(D|A) = \sum_\{i=1\}^n \frac{|D_i|}{|D|} H(D_i) \tag{5.2}
$$

* **直观理解**：这就是**“加权平均熵”**。算出每个子集的熵，然后按子集的大小（概率）加权求和。

#### 计算信息增益 (Information Gain)
信息增益 $g(D, A)$ 表示得知特征 $A$ 的信息而使得类 $D$ 的信息不确定性减少的程度。

$$
g(D, A) = H(D) - H(D|A) \tag{5.3}
$$

* **物理意义**：
    * **$H(D)$**：原始的混乱程度（不确定性）。
    * **$H(D|A)$**：经过特征 $A$ 分类后的混乱程度。
    * **$g(D, A)$**：**混乱程度减少了多少**。
* **决策准则**：信息增益越大，说明特征 $A$ 的分类能力越强，我们应该优先选择它作为切分点。



## 2.决策树

### 实例推导：贷款申请数据的特征选择

我们使用 ID3 算法，通过计算**信息增益**来选择根节点的特征。

**准备工作：**
* **数据集 $D$**：共 15 个样本。
* **类别 $Y$**：
    * $Y=\text{是}$（9个）
    * $Y=\text{否}$（6个）

#### 第一步：计算集合 $D$ 的经验熵 $H(D)$

$$
H(D) = -\left( \frac{9}{15}\log_2\frac{9}{15} + \frac{6}{15}\log_2\frac{6}{15} \right) = 0.971
$$

---

#### 第二步：计算各特征的信息增益

我们需要分别计算 4 个特征（年龄、有工作、有房子、信贷情况）的信息增益。

**1. 特征 $A_1$：年龄 (Age)**
* **切分情况**：
    * 青年 ($D_1$)：5 人（2 是，3 否）
    * 中年 ($D_2$)：5 人（3 是，2 否）
    * 老年 ($D_3$)：5 人（4 是，1 否）
* **计算条件熵 $H(D|A_1)$**：
    $$
    \begin{aligned}
    H(D_1) &= -(\frac{2}{5}\log_2\frac{2}{5} + \frac{3}{5}\log_2\frac{3}{5}) = 0.971 \\
    H(D_2) &= -(\frac{3}{5}\log_2\frac{3}{5} + \frac{2}{5}\log_2\frac{2}{5}) = 0.971 \\
    H(D_3) &= -(\frac{4}{5}\log_2\frac{4}{5} + \frac{1}{5}\log_2\frac{1}{5}) = 0.722
    \end{aligned}
    $$
    $$
    H(D|A_1) = \frac{5}{15}(0.971) + \frac{5}{15}(0.971) + \frac{5}{15}(0.722) = 0.888
    $$
* **信息增益**：
    $$g(D, A_1) = 0.971 - 0.888 = 0.083$$

**2. 特征 $A_2$：有工作 (Job)**
* **切分情况**：
    * 是 ($D_1$)：5 人（5 是，0 否）$\rightarrow$ **纯度极高，熵为 0**
    * 否 ($D_2$)：10 人（4 是，6 否）
* **计算条件熵 $H(D|A_2)$**：
    $$
    \begin{aligned}
    H(D_1) &= 0 \\
    H(D_2) &= -(\frac{4}{10}\log_2\frac{4}{10} + \frac{6}{10}\log_2\frac{6}{10}) = 0.971
    \end{aligned}
    $$
    $$
    H(D|A_2) = \frac{5}{15}(0) + \frac{10}{15}(0.971) = 0.647
    $$
* **信息增益**：
    $$g(D, A_2) = 0.971 - 0.647 = 0.324$$

**3. 特征 $A_3$：有自己的房子 (House)**
* **切分情况**：
    * 是 ($D_1$)：6 人（6 是，0 否）$\rightarrow$ **纯度极高，熵为 0**
    * 否 ($D_2$)：9 人（3 是，6 否）
* **计算条件熵 $H(D|A_3)$**：
    $$
    \begin{aligned}
    H(D_1) &= 0 \\
    H(D_2) &= -(\frac{3}{9}\log_2\frac{3}{9} + \frac{6}{9}\log_2\frac{6}{9}) = 0.918
    \end{aligned}
    $$
    $$
    H(D|A_3) = \frac{6}{15}(0) + \frac{9}{15}(0.918) = 0.551
    $$
* **信息增益**：
    $$g(D, A_3) = 0.971 - 0.551 = 0.420$$

**4. 特征 $A_4$：信贷情况 (Credit)**
* **切分情况**：
    * 非常好 ($D_1$)：4 人（4 是，0 否）$\rightarrow$ **熵为 0**
    * 好 ($D_2$)：6 人（4 是，2 否）
    * 一般 ($D_3$)：5 人（1 是，4 否）
* **计算条件熵 $H(D|A_4)$**：
    $$
    \begin{aligned}
    H(D_1) &= 0 \\
    H(D_2) &= 0.918 \\
    H(D_3) &= 0.722
    \end{aligned}
    $$
    $$
    H(D|A_4) = \frac{4}{15}(0) + \frac{6}{15}(0.918) + \frac{5}{15}(0.722) = 0.608
    $$
* **信息增益**：
    $$g(D, A_4) = 0.971 - 0.608 = 0.363$$

---

#### 第三步：比较与选择

我们将所有特征的信息增益汇总对比：

| 特征 | 信息增益 $g(D, A)$ |
| :--- | :--- |
| 年龄 ($A_1$) | 0.083 |
| 有工作 ($A_2$) | 0.324 |
| **有房子 ($A_3$)** | **0.420 (Max)** |
| 信贷情况 ($A_4$) | 0.363 |

**结论：**
由于特征 $A_3$（有自己的房子）的信息增益最大，所以我们在决策树的根节点选择 **“有自己的房子”** 作为切分特征。

* **左分支（有房子=是）**：所有 6 个样本全是“是”，分类结束，成为叶子节点。
* **右分支（有房子=否）**：包含 9 个样本（3 是，6 否），需要对这 9 个样本递归执行上述步骤，继续寻找下一个最优特征（通常是“有工作”）。

* **样本集合**：$D_2$ 包含 9 个样本，ID 为 $\{1, 2, 3, 5, 6, 7, 13, 14, 15\}$。
* **类别分布**：
    * $C_1$ (是)：3 个 $\{3, 13, 14\}$
    * $C_2$ (否)：6 个 $\{1, 2, 5, 6, 7, 15\}$
* **可用特征**：$A_1$ (年龄), $A_2$ (有工作), $A_4$ (信贷情况)。

#### 2. 计算 $D_2$ 的经验熵 $H(D_2)$

$$
H(D_2) = -\sum_{k=1}^K \frac{|C_k|}{|D_2|} \log_2 \frac{|C_k|}{|D_2|}
$$

代入数据计算：

$$
H(D_2) = -\left( \frac{3}{9}\log_2\frac{3}{9} + \frac{6}{9}\log_2\frac{6}{9} \right) = 0.918
$$

#### 3. 计算各特征的信息增益

我们需要计算剩余特征 $A_1, A_2, A_4$ 在 $D_2$ 上的信息增益。

**(1) 特征 $A_1$：年龄 (Age)**

将 $D_2$ 按年龄切分为三个子集：
* **青年 ($D_{21}$)**：4 个样本 $\{1, 2, 3, 5\}$ $\rightarrow$ (1 是, 3 否)
* **中年 ($D_{22}$)**：2 个样本 $\{6, 7\}$ $\rightarrow$ (0 是, 2 否)
* **老年 ($D_{23}$)**：3 个样本 $\{13, 14, 15\}$ $\rightarrow$ (2 是, 1 否)

计算各子集的熵：

$$
\begin{aligned}
H(D_{21}) &= -\left( \frac{1}{4}\log_2\frac{1}{4} + \frac{3}{4}\log_2\frac{3}{4} \right) = 0.811 \\\\
H(D_{22}) &= 0 \quad (\text{纯度 100\\%}) \\\\
 H(D_{23}) &= -\left( \frac{2}{3}\log_2\frac{2}{3} + \frac{1}{3}\log_2\frac{1}{3} \right) = 0.918
\end{aligned}
$$

计算条件熵 $H(D_2|A_1)$：

$$
\begin{aligned}
H(D_2|A_1) &= \sum_{i=1}^3 \frac{|D_{2i}|}{|D_2|} H(D_{2i}) \\
&= \frac{4}{9}(0.811) + \frac{2}{9}(0) + \frac{3}{9}(0.918) = 0.667
\end{aligned}
$$

计算信息增益 $g(D_2, A_1)$：

$$
g(D_2, A_1) = H(D_2) - H(D_2|A_1) = 0.918 - 0.667 = 0.251
$$

**(2) 特征 $A_2$：有工作 (Job)**

将 $D_2$ 按有无工作切分为两个子集：
* **是 ($D_{21}$)**：3 个样本 $\{3, 13, 14\}$ $\rightarrow$ (3 是, 0 否)。纯度极高，熵为 0。
* **否 ($D_{22}$)**：6 个样本 $\{1, 2, 5, 6, 7, 15\}$ $\rightarrow$ (0 是, 6 否)。纯度极高，熵为 0。

计算条件熵 $H(D_2|A_2)$：

$$
H(D_2|A_2) = \frac{3}{9}(0) + \frac{6}{9}(0) = 0
$$

计算信息增益 $g(D_2, A_2)$：

$$
g(D_2, A_2) = H(D_2) - H(D_2|A_2) = 0.918 - 0 = 0.918
$$

**(3) 特征 $A_4$：信贷情况 (Credit)**

将 $D_2$ 按信贷情况切分为三个子集：
* **非常好 ($D_{21}$)**：1 个样本 $\rightarrow$ (1 是)。$H=0$。
* **好 ($D_{22}$)**：4 个样本 $\rightarrow$ (2 是, 2 否)。$H=1$。
* **一般 ($D_{23}$)**：4 个样本 $\rightarrow$ (0 是, 4 否)。$H=0$。

计算条件熵 $H(D_2|A_4)$：

$$
H(D_2|A_4) = \frac{1}{9}(0) + \frac{4}{9}(1) + \frac{4}{9}(0) = 0.444
$$

计算信息增益 $g(D_2, A_4)$：

$$
g(D_2, A_4) = 0.918 - 0.444 = 0.474
$$

#### 4. 特征选择

对比各特征的信息增益：

$$
g(D_2, A_2) = 0.918 \quad > \quad g(D_2, A_4) = 0.474 \quad > \quad g(D_2, A_1) = 0.251
$$

**结论**：选择特征 $A_2$（有工作）作为 $D_2$ 节点的切分特征。
由于 $A_2$ 切分后的两个子集熵均为 0，决策树生长停止。

---

### 最终生成的决策树

最终的决策规则树结构如下：

$$
\text{Root} (A_3: \text{有房子?})
\begin{cases}
\text{是} \xrightarrow{\quad} \text{叶节点: 同意贷款} \\\\
\text{否} \xrightarrow{\quad} \text{Node}_2 (A_2: \text{有工作?})
    \begin{cases}
    \text{是} \xrightarrow{\quad} \text{叶节点: 同意贷款} \\\\
    \text{否} \xrightarrow{\quad} \text{叶节点: 拒绝贷款}
    \end{cases}
\end{cases}
$$

## 3.惩罚（剪枝）

## 4.回归树
## 5.分类树

## 6.Bagging

> 训练集和测试集

设想现在有一个数据集，有1000个样本，常规机器学习会在一开始就会把样本分为两份，一份用于训练，一份用于测试训练好的模型用于评判模型好坏，如果全用于测试然后再从样本中取一些拿来测试那就没什么意义了，就像考试总要留两套没做过的卷子拿来看看自己的水平。

> 单棵决策树存在的问题

回到决策树，如果把所有属性都考虑进去，那就会过拟合，因为这样会产生无数条规则，如果训练集中有一个特例，一个极端的例子就是一个人啥都没有还把贷款给他了，这显然是一个离群点，这个模型会认为这样的情况可以贷给他，这会造成不准确。不像线性模型，预测出的值不会离实际值太远，但是基于规则的决策树因为太精准，更容易过拟合，甚至是有些规则不可信，树越深规则越精细，越容易过拟合导致错误。导致和实际值差距特别大，所以有了剪枝操作。

> bagging思想

但是剪枝操作会损失信息，你可能会想，我辛苦收集了那么多样本信息，剪枝给我剪了一大半，不就是白干了吗，这里引入bagging思想，既然树可以这么深，那我就多训练几棵深的树，然后把每个样本数据分别交给这几棵树预测，当然，每棵树的预测值都不同，然后将这几棵树的结果平均一下，误差就小了。极端的例子就是，一个极度保守的树和一个极度激进的树，单靠任何一棵树预测出来的值都不可取，如果将他们的值平均一下，就会很温和了。这个道理其实和统计学的方差公式思想一样。

> Bootstrap

但是这会引出另一个问题，前面提到一个数据集就训练出一颗决策树，怎么能训练出多颗决策树，这里就要引入Bootstrap思想。我们把训练集比如有800个样本，在训练的时候随机抽取一个样本加入训练集，然后放回去，然后往复800次这个操作，最终得到800个训练集样本。自此训练出一颗决策树，然后根据想要训练的树的数量用相同方法重复最终得到n棵决策树。具体步骤后续解释

## 7. 随机森林 (Random Forest)

决策树虽然易于理解，但它有一个致命的缺点：**容易过拟合**。如果我们不限制树的生长，它会记住训练集中的所有噪声，导致泛化能力差。

随机森林 (Random Forest, RF) 是一种 **Bagging (Bootstrap Aggregating)** 集成算法。它通过构建多棵决策树，并将它们的预测结果进行汇总（分类问题取**众数**，回归问题取**平均**），从而得到一个更稳健的模型。

### 6.1 核心公式

假设我们训练了 $T$ 棵树 $\{h_1(x), h_2(x), \dots, h_T(x)\}$：

* 回归问题 (Regression)：
    $$
    H(x) = \frac{1}{T} \sum_{t=1}^T h_t(x)
    $$
* 分类问题 (Classification)：
    $$
    H(x) = \mathop{\arg\max}_{y} \sum_\{t=1}^T \mathbb{I}(h_t(x) = y)
    $$
    *(即：少数服从多数，投票表决)*

### 6.2 “双重随机性” (The Two Randoms)

随机森林之所以叫“随机”，是因为它在构建每棵树时引入了两个维度的随机性。这也是它抗过拟合的关键。

#### 1. 样本的随机性：Bootstrap Sampling (行采样)
对于第 $t$ 棵树，我们不是使用全部原始训练集 $D$，而是从 $D$ 中进行 **有放回抽样 (Sampling with Replacement)**。
* 假设 $D$ 有 $m$ 个样本。
* 我们要抽取 $m$ 次，形成一个新的数据集 $D_t$。
* 因为是有放回的，某些样本可能在 $D_t$ 中出现多次，而某些样本可能一次都没出现（参见下文 OOB）。

#### 2. 特征的随机性：Feature Subsampling (列采样)
在决策树的**每一个节点分裂**时，我们不考虑所有 $d$ 个特征，而是随机选择一个特征子集 $k$。
* 通常取 $k = \sqrt{d}$ 或 $k = \log_2 d$。
* 然后在选出的 $k$ 个特征中，寻找最优切分点。

> **💡 为什么要限制特征？**
> 如果不限制特征，如果数据集中有一个超强特征（例如“是否负债”对贷款极其重要），那么所有的树都会优先选这个特征做根节点。
> 这样训练出来的树长得都差不多（**相关性太高**）。
> **随机选特征是为了强制让树变得“多样化” (Decorrelated)。** 只有树之间差异够大，投票才有意义。



---

### 6.3 袋外错误率 (Out-of-Bag Error)

在使用 Bootstrap 采样时，有一个有趣的数学性质：大约有 **36.8%** 的原始样本不会出现在采样集 $D_t$ 中。这些数据被称为 **袋外数据 (Out-of-Bag, OOB)**。

#### 数学推导
假设数据集有 $m$ 个样本。每次抽取选中某个特定样本的概率是 $\frac{1}{m}$，没选中的概率是 $1 - \frac{1}{m}$。
抽取 $m$ 次后，该样本始终没被选中的概率是：

$$
P(\text{未被选中}) = \left( 1 - \frac{1}{m} \right)^m
$$

当 $m \to \infty$ 时，根据重要极限公式：

$$
\lim_{m \to \infty} \left( 1 - \frac{1}{m} \right)^m = \frac{1}{e} \approx \frac{1}{2.718} \approx 0.368
$$

#### OOB 的作用
* **自带验证集**：由于这 36.8% 的数据没有参与第 $t$ 棵树的训练，我们可以直接用这棵树来预测它们，评估误差。
* **替代交叉验证**：在随机森林中，我们通常不需要再单独切分验证集，OOB Error 是对泛化误差的一个无偏估计。

---

### 6.4 随机森林算法流程

1.  输入：数据集 $D$，树的数量 $T$，特征子集大小 $k$。
2.  对于 $t = 1$ 到 $T$：
    * **Bootstrap**：从 $D$ 中有放回抽取 $m$ 个样本，得到 $D_t$。
    * **训练**：用 $D_t$ 训练一棵决策树 $h_t$。在树的每个节点分裂时：
        * 随机从所有 $d$ 个特征中选择 $k$ 个特征。
        * 在这 $k$ 个特征中选择最优切分点。
        * 让树生长到最大深度（通常不剪枝）。
3.  输出：组合 $T$ 棵树的结果。

---

### 6.5 优缺点总结

| 优点 | 缺点 |
| :--- | :--- |
| **抗过拟合**：比单棵树稳健得多。 | **模型庞大**：需要存储很多棵树，空间占用大。 |
| **并行化**：每棵树互不依赖，可以并行训练（速度快）。 | **预测慢**：预测时需要遍历所有树。 |
| **无需特征缩放**：和决策树一样，不敏感。 | **黑盒模型**：丧失了单棵树的可解释性（看不出具体规则）。 |
| **自带特征重要性**：可以通过 OOB 数据计算特征的重要性排序。 | |

## 7.Boosting

